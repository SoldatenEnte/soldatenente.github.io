<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <link rel="icon" href="/favicon.ico" sizes="any"><link rel="icon" href="/favicon.svg" type="image/svg+xml"><link rel="apple-touch-icon" href="/apple-touch-icon.png">
    <title>Unstable Diffusion: An AI-Driven Animation Experiment | Generative Art Case Study</title>
    <meta name="description" content="A case study on an experimental animation pipeline using Stable Diffusion and ControlNet, guided by a depth map sequence from Blender."/>
    <link rel="canonical" href="https://ducklin.de/case-studies/unstable-diffusion.html" />
    <meta property="og:title" content="Unstable Diffusion: An AI-Driven Animation Experiment | Generative Art Case Study" />
    <meta property="og:description" content="A case study on an experimental animation pipeline using Stable Diffusion and ControlNet, guided by a depth map sequence from Blender." />
    <meta property="og:type" content="article" /><meta property="og:url" content="https://ducklin.de/case-studies/unstable-diffusion.html" />
    <meta property="og:image" content="https://ducklin.de/images/unstable-diffusion_0.webp" />
    <meta property="og:site_name" content="Ducklin Portfolio" /><meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Unstable Diffusion: An AI-Driven Animation Experiment | Generative Art Case Study" />
    <meta name="twitter:description" content="A case study on an experimental animation pipeline using Stable Diffusion and ControlNet, guided by a depth map sequence from Blender." />
    <meta name="twitter:image" content="https://ducklin.de/images/unstable-diffusion_0.webp" />
    <script type="application/ld+json">{"@context":"https://schema.org","@type":"TechArticle","headline":"Unstable Diffusion: An AI-Driven Animation Experiment","description":"A case study on an experimental animation pipeline using Stable Diffusion and ControlNet, guided by a depth map sequence from Blender.","image":"https://ducklin.de/images/unstable-diffusion_0.webp","author":{"@type":"Person","name":"Aaron Postels","url":"https://ducklin.de"}}</script>
    <link rel="stylesheet" href="../css/style.css" />
    <link rel="stylesheet" href="../css/case-study.css" />
</head>
<body>
    <header class="case-study-header" style="--bg-image: url('/images/unstable-diffusion_0.webp');">
        <h1>Unstable Diffusion</h1><p>An AI-Driven Animation Experiment</p>
        <div class="case-study-links"><a href="/" class="btn btn-secondary">&larr; Back to Portfolio</a></div>
    </header>
    <main class="case-study-main">
        <section><h2>Project Overview</h2><p>The vision for "Unstable Diffusion" was to explore the creative potential of generative AI as an interpretive tool. The project tests the AI's ability to perceive and construct recognizable forms, specifically human faces and figures, from purely abstract and ambiguous motion data provided by a depth map. The goal was to generate a unique, fluid animation where human-like forms appear to emerge and dissolve from a chaotic source.</p></section>
        
        <section class="case-study-gallery">
            <h2>Gallery</h2>
            <div class="gallery-grid">
                <a href="/images/unstable-diffusion_0.webp">
                    <img src="/images/unstable-diffusion_0.webp" alt="Abstract 3D form used as AI input." loading="lazy" width="600">
                </a>
            </div>
        </section>

        <section><h2>Tech Stack</h2><ul class="tech-stack-list"><li>Stable Diffusion</li><li>ControlNet</li><li>Blender</li><li>Python</li></ul></section>
        
        <section><h2>Key Features</h2><ul><li><strong>Abstract Animation:</strong> An abstract, continuously deforming 3D sculpture was created and animated in Blender to serve as the motion source.</li><li><strong>Depth Map Generation:</strong> The animation was rendered as a sequence of depth map images, creating a video that encodes spatial information without recognizable features.</li><li><strong>AI Interpretation with ControlNet:</strong> The depth map video was fed into a Stable Diffusion model using ControlNet, which forced the AI to adhere to the structure and motion of the depth map.</li><li><strong>Final Animation:</strong> By prompting the AI to generate human faces and figures, the process resulted in a surreal animation where abstract shapes fluidly resolve into recognizable, yet constantly shifting, human-like forms.</li></ul></section>
    </main>
    <footer>&copy; 2025 Aaron Postels @ Ducklin &mdash;<a href="https://github.com/SoldatenEnte" target="_blank" rel="noopener noreferrer">GitHub</a> &mdash;<a href="/imprint.html">Legal Notice</a></footer>

    <div class="lightbox-overlay" id="lightbox">
        <div class="lightbox-content">
            <img src="" alt="Enlarged gallery image" id="lightbox-img">
        </div>
        <button class="lightbox-close" id="lightbox-close" aria-label="Close image viewer">&times;</button>
    </div>
    <script src="../js/case-study.js" defer></script>
</body>
</html>